{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "### First, what is reinforcement learning?\n",
    "Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "\n",
    "![reinforcement_learning](https://images.ecosia.org/lisjdPOy_rATnKdUqSj2IFDjD_I=/0x390/smart/https%3A%2F%2Fi.stack.imgur.com%2FeoeSq.png)\n",
    "\n",
    "### deep reinforcement learning = reinforcement learning + deep learning\n",
    "In 2013 Google DeepMind published the paper [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602), where they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. This was done using a new algorithm called **Deep Q Network (DQN)**.\n",
    "\n",
    "Deep learning plays its part because instead of using the classic **Q function** *Q(s,a)*, which returns the expected value if from the **current state** *s* the agent performs the **action** *a*, in Deep Q-learning there is a **neural network** used to approximate this value.\n",
    "\n",
    "### The example in this notebook\n",
    "![cartpole](https://images.ecosia.org/xOJ8LOC_xNbxq2fyI-bXyD1Fnq8=/0x390/smart/https%3A%2F%2Frubenfiszel.github.io%2Fposts%2Frl4j%2Fcartpole.gif)\n",
    "\n",
    "This is one of the most known example of reinforcement learning. The goal is balancing a pole on top of a moving cart.\n",
    "Other great examples can be found on the [**OpenAI Gym** website](https://gym.openai.com/envs/#classic_control).\n",
    "\n",
    "### Credits for the code to [keon.io](https://keon.io/deep-q-learning/)\n",
    "\n",
    "I followed his tutorial to create my first agent trained with reinforcement learning. I slightly modified the code and added some comments. More examples coming soon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of games\n",
    "EPISODES = 1000\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95   # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995 # while the agent is improving we decrease the exploration rate\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    '''This is the neural network used to approximate the reward value.'''\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    '''Using a neural network to train the agent has its drawbacks. The main problem is that it tends to forget the\n",
    "    previous experiences. That s why we need a way to store these previous experiences such that we can reuse them\n",
    "    later using the replay function.'''\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    '''Initially the agent doesn t know what to do, so it acts randomly (if the extracted value is <= of the exploration rate).\n",
    "    After a while, it will start predicting the reward value based on the current state and then returns the action which\n",
    "    maximize this value.'''\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    '''This function simply extracts memories of previous experiences from the memory of the agent and splits them\n",
    "    in the so called mini-batches, used then for the training phase.'''\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 11, e: 1.0\n",
      "episode: 1/1000, score: 18, e: 1.0\n",
      "episode: 2/1000, score: 16, e: 0.93\n",
      "episode: 3/1000, score: 18, e: 0.85\n",
      "episode: 4/1000, score: 10, e: 0.81\n",
      "episode: 5/1000, score: 9, e: 0.77\n",
      "episode: 6/1000, score: 14, e: 0.72\n",
      "episode: 7/1000, score: 67, e: 0.51\n",
      "episode: 8/1000, score: 160, e: 0.23\n",
      "episode: 9/1000, score: 98, e: 0.14\n",
      "episode: 10/1000, score: 85, e: 0.092\n",
      "episode: 11/1000, score: 44, e: 0.074\n",
      "episode: 12/1000, score: 130, e: 0.038\n",
      "episode: 13/1000, score: 114, e: 0.022\n",
      "episode: 14/1000, score: 176, e: 0.01\n",
      "episode: 15/1000, score: 163, e: 0.01\n",
      "episode: 16/1000, score: 207, e: 0.01\n",
      "episode: 17/1000, score: 343, e: 0.01\n",
      "episode: 18/1000, score: 157, e: 0.01\n",
      "episode: 19/1000, score: 188, e: 0.01\n",
      "episode: 20/1000, score: 499, e: 0.01\n",
      "episode: 21/1000, score: 173, e: 0.01\n",
      "episode: 22/1000, score: 111, e: 0.01\n",
      "episode: 23/1000, score: 53, e: 0.01\n",
      "episode: 24/1000, score: 9, e: 0.01\n",
      "episode: 25/1000, score: 26, e: 0.01\n",
      "episode: 26/1000, score: 9, e: 0.01\n",
      "episode: 27/1000, score: 289, e: 0.01\n",
      "episode: 28/1000, score: 499, e: 0.01\n",
      "episode: 29/1000, score: 74, e: 0.01\n",
      "episode: 30/1000, score: 499, e: 0.01\n",
      "episode: 31/1000, score: 499, e: 0.01\n",
      "episode: 32/1000, score: 158, e: 0.01\n",
      "episode: 33/1000, score: 201, e: 0.01\n",
      "episode: 34/1000, score: 189, e: 0.01\n",
      "episode: 35/1000, score: 284, e: 0.01\n",
      "episode: 36/1000, score: 157, e: 0.01\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episodes = 1000\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "#agent.load(\"./save/cartpole-dqn.h5\")\n",
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    #if e % 10 == 0:\n",
    "    #    agent.save(\"./save/cartpole-dqn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
